### YamlMime:FAQ
metadata:
  title: 'Azure Data Factory: Häufig gestellte Fragen '
  description: Hier finden Sie Antworten auf häufig gestellte Fragen zu Azure Data Factory.
  author: ssabat
  ms.author: susabat
  ms.service: data-factory
  ms.subservice: 
  ms.topic: conceptual
  ms.date: 07/23/2021
  ms.openlocfilehash: 792d61cf017fc098db92b0ac1ec4670a08456069
  ms.sourcegitcommit: f6e2ea5571e35b9ed3a79a22485eba4d20ae36cc
  ms.translationtype: HT
  ms.contentlocale: de-DE
  ms.lasthandoff: 09/24/2021
  ms.locfileid: "128571379"
title: 'Azure Data Factory: Häufig gestellte Fragen'
summary: "[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]\n\nDieser Artikel enthält Antworten auf häufig gestellte Fragen zu Azure Data Factory.  \n"
sections:
- name: Wird ignoriert.
  questions:
  - question: >
      Was ist Azure Data Factory?
    answer: "Data Factory ist ein vollständig verwalteter, cloudbasierter ETL-Datenintegrationsdienst, der das Verschieben und Transformieren von Daten automatisiert. Genau wie ein Betrieb, in dem Anlagen Rohmaterialien in Endprodukte umwandeln, organisiert Azure Data Factory vorhandene Dienste so, dass Rohdaten gesammelt und in nutzbare Informationen transformiert werden. \n\nSie können mithilfe von Azure Data Factory datengesteuerte Workflows zum Verschieben von Daten zwischen lokalen Datenspeichern und Clouddatenspeichern erstellen. Darüber hinaus können Sie Daten mit Datenflüssen verarbeiten und transformieren. ADF unterstützt außerdem mithilfe von Computediensten wie Azure HDInsight, Azure Databricks und SQL Server Integration Services (SSIS) Integration Runtime externe Computeengines für manuell programmierte Transformationen.\n\nMit Data Factory können Sie die Datenverarbeitung in einem Azure-basierten Clouddienst oder in Ihrer eigenen selbstgehosteten Compute-Umgebung wie SSIS, SQL Server oder Oracle ausführen. Nachdem Sie eine Pipeline erstellt haben, die die gewünschte Aktion ausführt, können Sie die regelmäßige Ausführung der Pipeline planen (beispielsweise stündlich, täglich oder wöchentlich), einen Plan mit Zeitfenstern aufstellen oder die Pipeline über ein Ereignis auslösen. Weitere Informationen finden Sie unter [Einführung in Azure Data Factory](introduction.md).\n"
  - question: >
      Überlegungen zu Compliance und Sicherheit
    answer: "Azure Data Factory erfüllt eine Reihe von Compliancezertifizierungen, einschließlich _SOC 1, 2, 3_, _HIPAA BAA_ und _HITRUST_. Eine vollständige (und ständig wachsende) Liste der Zertifizierungen finden Sie [hier](data-movement-security-considerations.md). Digitale Kopien für Überwachungsberichte und Compliancezertifizierungen finden Sie im [Service Trust Center](https://servicetrust.microsoft.com/).\n\n### <a name=\"control-flows-and-scale\"></a>Ablaufsteuerungen und Skalierung\n\nZur Unterstützung der vielfältigen Integrationsabläufe und -muster eines modernen Data Warehouse ermöglicht Data Factory eine flexible Modellierung der Datenpipeline. Dies beinhaltet umfassende Paradigmen für die Programmierung der Ablaufsteuerung. Hierzu zählen etwa bedingte Ausführung, Verzweigung in Datenpipelines und die Möglichkeit zum expliziten Übergeben von Parametern für diese Abläufe (intern und übergreifend). Außerdem umfasst die Ablaufsteuerung die Transformation von Daten per Aktivitätsverteilung auf externe Ausführungsmodule und Datenflussfunktionen (beispielsweise bedarfsabhängige Datenverschiebungen) über die Kopieraktivität.\n\nMit Data Factory können Sie nun flexibel einen beliebigen Ablaufstil modellieren, der für die Datenintegration benötigt wird und entweder bedarfsabhängig oder nach einem bestimmten Zeitplan angewendet werden soll. Einige allgemeine Abläufe, die mit diesem Modell ermöglicht werden, sind:\n\n- Ablaufsteuerungen:\n    - Aktivitäten können innerhalb einer Pipeline zu einer Sequenz verkettet werden.\n    - Aktivitäten können innerhalb einer Pipeline verzweigt werden.\n    - Parameter:\n        * Parameter können auf der Pipelineebene definiert und Argumente beim bedarfs- oder triggergesteuerten Aufrufen der Pipeline übergeben werden.\n        * Aktivitäten können die an die Pipeline übergebenen Argumente nutzen.\n    - Übergeben von benutzerdefinierten Zuständen:\n        * Aktivitätsausgaben (einschließlich des Zustands) können von einer Folgeaktivität in der Pipeline genutzt werden.\n    - Schleifencontainer:\n        * Die ForEach-Aktivität durchläuft eine bestimmte Sammlung von Aktivitäten in einer Schleife. \n- Triggerbasierte Abläufe:\n    - Pipelines können nach Bedarf, nach der regulären Uhrzeit (Ortszeit) oder als Reaktion auf die von Event Grid-Themen gesteuerten Informationen ausgelöst werden.\n- Deltaabläufe:\n    - Sie können mithilfe von Parametern die Obergrenze für Deltakopien beim Verschieben von Dimensions- oder Referenztabellen aus einem relationalen Speicher (entweder lokal oder in der Cloud) definieren, um die Daten in den Lake zu laden.\n\nWeitere Informationen finden Sie im [Tutorial: Ablaufsteuerungen](tutorial-control-flow.md).\n\n### <a name=\"data-transformed-at-scale-with-code-free-pipelines\"></a>Nach Bedarf transformierte Daten mit Pipelines ohne Code\n\nDie neue browserbasierte Toolumgebung ermöglicht die Erstellung und Bereitstellung von Pipelines ohne Code mit einer modernen, interaktiven webbasierten Benutzeroberfläche.\n\nFür Entwickler visueller Daten und Datentechniker ist die Data Factory-Webbenutzeroberfläche die codefreie Entwurfsumgebung für die Pipelineerstellung. Sie ist vollständig in Visual Studio Online Git integriert und ermöglicht die Integration für CI/CD sowie die iterative Entwicklung mit Debugoptionen.\n\n### <a name=\"rich-cross-platform-sdks-for-advanced-users\"></a>Umfassende plattformübergreifende SDKs für erfahrene Benutzer\n\nData Factory V2 bietet verschiedenste SDKs für die Erstellung, Verwaltung und Überwachung von Pipelines in Ihrer bevorzugten IDE:\n\n* Python SDK\n* PowerShell CLI\n* C# SDK\n\nBenutzer können außerdem die dokumentierten REST-APIs verwenden, um mit Data Factory V2 zu interagieren.\n\n### <a name=\"iterative-development-and-debugging-by-using-visual-tools\"></a>Iteratives Entwickeln und Debuggen mit visuellen Tools\n\nDie visuellen Tools von Azure Data Factory ermöglichen iteratives Entwickeln und Debuggen. Mit der Funktion **Debuggen** auf der Pipelinecanvas können Sie Pipelines erstellen und Testläufe durchführen, ohne eine einzige Codezeile zu schreiben. Die Ergebnisse der Testläufe können auf der Pipelinecanvas im Fenster **Ausgabe** angezeigt werden. Nach erfolgreicher Durchführung des Testlaufs können Sie der Pipeline weitere Aktivitäten hinzufügen und das iterative Debuggen fortsetzen. Bereits gestartete Testläufe können abgebrochen werden.\n\nSie müssen Ihre Änderungen nicht für den Data Factory-Dienst veröffentlichen, bevor Sie **Debuggen** auswählen. Dies ist hilfreich, wenn Sie sich vergewissern möchten, dass die neuen Elemente oder Änderungen wie erwartet funktionieren, bevor Sie Ihre Data Factory-Workflows in Entwicklungs-, Test- oder Produktionsumgebungen aktualisieren.\n\n### <a name=\"ability-to-deploy-ssis-packages-to-azure\"></a>Möglichkeit zum Bereitstellen von SSIS-Paketen in Azure\n\nWenn Sie Ihre SSIS-Workloads verschieben möchten, können Sie eine Data Factory-Instanz erstellen und eine Azure-SSIS-Integration Runtime bereitstellen. Eine Azure-SSIS Integration Runtime ist ein vollständig verwalteter Cluster mit virtuellen Azure-Computern (Knoten), die speziell für die Ausführung von SSIS-Paketen in der Cloud bestimmt sind. Eine ausführliche Anleitung finden Sie im Tutorial [Bereitstellen von SSIS-Paketen in Azure](./tutorial-deploy-ssis-packages-azure.md). \n\n### <a name=\"sdks\"></a>SDKs\n\nFür erfahrene Benutzer, die auf der Suche nach einer befehlsorientierten Benutzerschnittstelle sind, bietet Data Factory ein breites Spektrum an SDKs zum Erstellen, Verwalten und Überwachen von Pipelines mit Ihrer bevorzugten IDE. Die Sprachunterstützung umfasst .NET, PowerShell, Python und REST.\n\n### <a name=\"monitoring\"></a>Überwachung\n\nSie können Ihre Data Factorys per PowerShell, SDK oder mit den visuellen Überwachungstools auf der Benutzeroberfläche des Browsers überwachen. Sie können bedarfsabhängige, triggerbasierte und zeitgesteuerte benutzerdefinierte Datenflüsse auf effiziente und effektive Weise überwachen und verwalten. Brechen Sie vorhandene Aufgaben ab, nutzen Sie die Fehlerübersicht, zeigen Sie Detailinformationen an, um ausführliche Fehlermeldungen zu erhalten, und debuggen Sie alle Probleme an einem zentralen Ort, ohne den Kontext wechseln oder zwischen Bildschirmen navigieren zu müssen.\n\n### <a name=\"new-features-for-ssis-in-data-factory\"></a>Neue Features für SSIS in Data Factory\n\nSeit der ersten öffentlichen Vorschauversion im Jahr 2017 wurden mit Data Factory folgende Features für SSIS hinzugefügt:\n\n-    Unterstützung von drei weiteren Konfigurationen/Varianten von Azure SQL-Datenbank zum Hosten der SSIS-Datenbank (SSISDB) mit Projekten/Paketen:\n-    SQL-Datenbank mit VNET-Dienstendpunkten\n-    Verwaltete SQL-Instanz\n-    Pool für elastische Datenbanken\n-    Unterstützung eines virtuellen Azure Resource Manager-Netzwerks auf der Grundlage eines klassischen virtuellen Netzwerks, das irgendwann als veraltet eingestuft wird. Dadurch können Sie Ihre Azure-SSIS Integration Runtime in ein virtuelles Netzwerk integrieren, das für SQL-Datenbank mit VNET-Dienstendpunkten, einer verwalteten Instanz und lokalem Datenzugriff konfiguriert ist. Weitere Informationen finden Sie unter [Verknüpfen einer Azure-SSIS Integration Runtime mit einem virtuellen Netzwerk](join-azure-ssis-integration-runtime-virtual-network.md).\n-    Unterstützung von Azure AD-Authentifizierung (Azure Active Directory) und SQL-Authentifizierung für die Verbindungsherstellung mit der SSISDB, um die Azure AD-Authentifizierung mit Ihrer von Data Factory verwalteten Identität für Azure-Ressourcen zu ermöglichen\n-    Unterstützung der Verwendung Ihrer eigenen SQL Server-Lizenz zur Erzielung erheblicher Kosteneinsparungen über den Azure-Hybridvorteil\n-    Unterstützung der Enterprise Edition der Azure-SSIS Integration Runtime mit erweiterten Features, Premium-Features, einer Schnittstelle für das benutzerdefinierte Setup, um zusätzliche Komponenten/Erweiterungen zu installieren, sowie mit einem Partnerökosystem. Weitere Informationen finden Sie unter [Enterprise Edition, benutzerdefiniertes Setup und Erweiterbarkeit durch Drittanbieter für SSIS in ADF](https://blogs.msdn.microsoft.com/ssis/2018/04/27/enterprise-edition-custom-setup-and-3rd-party-extensibility-for-ssis-in-adf/). \n-    Tiefere Integration von SSIS in Data Factory, die Ihnen das Aufrufen bzw. Auslösen erstklassiger Aktivitäten zum Ausführen des SSIS-Pakets in Data Factory-Pipelines und die Planung per SSMS ermöglicht. Weitere Informationen finden Sie unter [Modernisieren und Erweitern von ETL/ELT-Workflows mit SSIS-Aktivitäten in ADF-Pipelines](https://blogs.msdn.microsoft.com/ssis/2018/05/23/modernize-and-extend-your-etlelt-workflows-with-ssis-activities-in-adf-pipelines/).\n"
  - question: >
      Was ist die Integration Runtime?
    answer: >
      Die Integration Runtime ist die Computeinfrastruktur, mit der Azure Data Factory die folgenden Datenintegrationsfunktionen für verschiedene Netzwerkumgebungen bereitstellt:


      - **Datenverschiebung:** Zur Datenverschiebung werden die Daten von der Integration Runtime zwischen dem Quell- und Zieldatenspeicher verschoben, während gleichzeitig Unterstützung für integrierte Connectors, Formatkonvertierungen, Spaltenzuordnungen und leistungsfähige sowie skalierbare Datenübertragungen bereitgestellt wird.

      - **Datenfluss** – Wählen Sie die Option [Datenfluss](./concepts-data-flow-overview.md) aus, um in einer verwalteten Azure-Computeumgebung einen Datenfluss auszuführen.

      - **Verteilungsaktivitäten:** Für die Transformation bietet die Integration Runtime Funktionen zur nativen Ausführung von SSIS-Paketen.

      - **SSIS-Pakete ausführen:** Die Integration Runtime führt SSIS-Pakete nativ in einer verwalteten Azure-Computeumgebung aus. Zudem unterstützt die Integration Runtime die Übermittlung und Überwachung von Transformationsaktivitäten, die in verschiedensten Computediensten wie Azure HDInsight, Azure Machine Learning, SQL-Datenbank und SQL Server ausgeführt werden.


      Sie können zum Verschieben und Transformieren von Daten je nach Bedarf eine oder mehrere Integration Runtime-Instanzen bereitstellen. Die Integration Runtime kann in einem öffentlichen Azure-Netzwerk oder in einem privaten Netzwerk (lokal, Azure Virtual Network oder Virtual Private Cloud (VPC) von Amazon Web Services) ausgeführt werden.

      In Data Factory wird mit einer Aktivität eine durchzuführende Aktion definiert. Mit einem verknüpften Dienst wird ein Zieldatenspeicher oder ein Computedienst definiert.

      Eine Integrationslaufzeit stellt die Brücke zwischen der Aktivität und verknüpften Diensten dar. Sie wird vom verknüpften Dienst oder der Aktivität referenziert und stellt die Computeumgebung bereit, in der die Aktivität entweder ausgeführt wird oder aus der sie verteilt wird. Auf diese Weise kann die Aktivität in der Region durchgeführt werden, die dem Zieldatenspeicher bzw. dem Computedienst am nächsten liegt, und es kann die höchste Leistung erzielt werden, während gleichzeitig die Anforderungen an die Sicherheit und Konformität erfüllt werden.


      Integration Runtimes sowie sämtliche Aktivitäten, Datasets oder Datenflüsse, die auf diese verweisen, können in der Benutzeroberfläche von Azure Data Factory über den Verwaltungshub erstellt werden.

      Weitere Informationen finden Sie unter [Integrationslaufzeit in Azure Data Factory](./concepts-integration-runtime.md).
  - question: >
      Welcher Grenzwert besteht hinsichtlich der Anzahl von Integrationslaufzeiten?
    answer: >
      Es gibt keine festen Grenzwerte hinsichtlich der Anzahl von Integration Runtime-Instanzen in einer Data Factory. Es gibt jedoch einen Grenzwert hinsichtlich der Anzahl von VM-Kernen, die von Integration Runtime pro Abonnement für die Ausführung von SSIS-Paketen verwendet werden können. Weitere Informationen finden Sie unter [Data Factory-Grenzwerte](../azure-resource-manager/management/azure-subscription-service-limits.md#data-factory-limits).
  - question: >
      Wie lauten die Hauptkonzepte von Azure Data Factory?
    answer: "Ein Azure-Abonnement kann über mindestens eine Azure Data Factory-Instanz (oder Data Factory) verfügen. Azure Data Factory enthält vier Hauptkomponenten, die zusammen als Plattform fungieren, auf der Sie datengesteuerte Workflows mit Schritten zum Verschieben und Transformieren von Daten zusammenstellen können.\n\n### <a name=\"pipelines\"></a>Pipelines\n\nEine Data Factory kann eine oder mehrere Pipelines haben. Bei einer Pipeline handelt es sich um eine logische Gruppierung von Aktivitäten zur Durchführung einer Arbeitseinheit. Gemeinsam führen die Aktivitäten einer Pipeline eine Aufgabe durch. Eine Pipeline kann beispielsweise eine Gruppe von Aktivitäten enthalten, die Daten aus einem Azure-Blob erfasst, und anschließend eine Hive-Abfrage in einem HDInsight-Cluster ausführen, um die Daten zu partitionieren. Der Vorteil ist, dass Sie eine Pipeline zum Verwalten der Aktivitäten als Gruppe verwenden können, statt jede Aktivität einzeln zu verwalten. Sie können die Aktivitäten in einer Pipeline miteinander verketten, um sie sequenziell auszuführen. Sie können sie aber auch unabhängig voneinander parallel ausführen.\n\n### <a name=\"data-flows\"></a>Datenflüsse\n\nBei Datenflüssen handelt es sich um Objekte, die Sie visuell in Data Factory erstellen und die Daten bedarfsorientiert für Spark-Back-End-Dienste transformieren. Sie müssen nicht mit der Programmierung oder mit Spark vertraut sein. Entwerfen Sie einfach Ihre Absicht für die Datentransformation mithilfe von Graphen (Mapping) oder Spreadsheets (Power Query-Aktivität).\n\n ### <a name=\"activities\"></a>activities\n\nAktivitäten stellen einen Verarbeitungsschritt in einer Pipeline dar. Beispielsweise können Sie eine Kopieraktivität verwenden, um Daten zwischen zwei Datenspeichern zu kopieren. Analog dazu können Sie eine Hive-Aktivität verwenden, die eine Hive-Abfrage für einen Azure HDInsight-Cluster ausführt, um Ihre Daten zu transformieren oder zu analysieren. Data Factory unterstützt drei Arten von Aktivitäten: Datenverschiebungsaktivitäten, Datentransformationsaktivitäten und Steuerungsaktivitäten.\n\n### <a name=\"datasets\"></a>Datasets\n\nDatasets stellen Datenstrukturen in den Datenspeichern dar, die einfach auf die Daten zeigen bzw. verweisen, die Sie in Ihren Aktivitäten als Eingaben oder Ausgaben verwenden möchten. \n\n### <a name=\"linked-services\"></a>Verknüpfte Dienste\n\nVerknüpfte Dienste ähneln Verbindungszeichenfolgen, mit denen die Verbindungsinformationen definiert werden, die für Data Factory zum Herstellen einer Verbindung mit externen Ressourcen erforderlich sind. Das können Sie sich wie folgt vorstellen: Ein verknüpfter Dienst definiert die Verbindung mit der Datenquelle, und ein Dataset stellt die Struktur der Daten dar. So gibt etwa ein verknüpfter Azure Storage-Dienst die Verbindungszeichenfolge an, um eine Verbindung mit einem Azure Storage-Konto herzustellen. Und ein Azure-Blobdataset gibt den Blobcontainer und den Ordner an, der die Daten enthält.\n\nVerknüpfte Dienste haben in Data Factory zwei Funktionen:\n\n- Sie stellen einen *Datenspeicher* dar, der beispielsweise eine SQL Server-Instanz, eine Oracle-Datenbankinstanz, eine Dateifreigabe oder ein Azure Blob Storage-Konto enthalten kann (er ist aber nicht darauf beschränkt). Eine Liste der unterstützten Datenspeicher finden Sie unter [Kopieraktivität in Azure Data Factory](copy-activity-overview.md).\n- Sie stellen eine *Computeressource* dar, die die Ausführung einer Aktivität hosten kann. So wird beispielsweise die HDInsight-Hive-Aktivität in einem HDInsight-Hadoop-Cluster ausgeführt. Eine Liste mit Transformationsaktivitäten und unterstützten Compute-Umgebungen finden Sie unter [Transformieren von Daten in Azure Data Factory](transform-data.md).\n\n### <a name=\"triggers\"></a>Trigger\n\nTrigger stellen Verarbeitungseinheiten dar, die bestimmen, wann eine Pipelineausführung initiiert wird. Es gibt verschiedene Arten von Triggern für unterschiedliche Arten von Ereignissen. \n\n### <a name=\"pipeline-runs\"></a>Pipelineausführungen\n\nEine Pipelineausführung ist eine Instanz einer ausgeführten Pipeline. Zur Instanziierung einer Pipelineausführung werden in der Regel die Argumente an die in Pipelines definierten Parameter übergeben. Die Argumente können manuell oder im Rahmen der Triggerdefinition übergeben werden.\n\n### <a name=\"parameters\"></a>Parameter\n\nParameter sind Schlüssel-Wert-Paare in einer schreibgeschützten Konfiguration. Sie definieren Parameter in einer Pipeline, und Sie übergeben die Argumente für die definierten Parameter während der Ausführung über einen Ausführungskontext. Der Ausführungskontext wird durch einen Trigger oder über eine Pipeline erstellt, die Sie manuell ausführen. Die Parameterwerte werden von Aktivitäten in der Pipeline genutzt.\n\nEin Dataset ist ein stark typisierter Parameter und eine Entität, die Sie wiederverwenden oder auf die Sie verweisen können. Eine Aktivität kann auf Datasets verweisen und die Eigenschaften nutzen, die in der Datasetdefinition festgelegt sind.\n\nBei einem verknüpften Dienst handelt es sich ebenfalls um einen stark typisierten Parameter mit Verbindungsinformationen für einen Datenspeicher oder eine Compute-Umgebung. Er ist auch eine Entität, die Sie wiederverwenden oder auf die Sie verweisen können.\n\n### <a name=\"control-flows\"></a>Ablaufsteuerungen\n\nAblaufsteuerungen orchestrieren Pipelineaktivitäten. Dies umfasst die Verkettung von Aktivitäten in einer Sequenz, Verzweigungen, auf Pipelineebene definierte Parameter sowie Argumente, die beim bedarfs- oder triggergesteuerten Aufrufen der Pipeline übergeben werden. Ablaufsteuerungen umfassen zudem das Übergeben von benutzerdefinierten Zuständen und Schleifencontainern (ForEach-Iteratoren).\n\n\nWeitere Informationen zu den Data Factory-Konzepten finden Sie in den folgenden Artikeln:\n\n- [Datasets und verknüpfte Dienste](concepts-datasets-linked-services.md)\n- [Pipelines und Aktivitäten](concepts-pipelines-activities.md)\n- [Integrationslaufzeit](concepts-integration-runtime.md)\n"
  - question: >
      Wie sieht das Preismodell für Data Factory aus?
    answer: >
      Preisinformationen zu Azure Data Factory finden Sie unter [Data Factory – Preisübersicht](https://azure.microsoft.com/pricing/details/data-factory/).
  - question: >
      Wie kann ich hinsichtlich der Informationen zu Data Factory auf dem neuesten Stand bleiben?
    answer: >
      Aktuelle Informationen zu Azure Data Factory finden Sie auf den folgenden Websites:


      - [Blog](https://azure.microsoft.com/blog/tag/azure-data-factory/)

      - [Startseite der Dokumentation](./index.yml)

      - [Produktstartseite](https://azure.microsoft.com/services/data-factory/)
  - question: >
      Technischer Einblick
    answer: "### <a name=\"how-can-i-schedule-a-pipeline\"></a>Wie kann ich eine Pipeline planen?\n\nSie können den Planer-Trigger oder Zeitfenster-Trigger verwenden, um eine Pipeline zu planen. Der Trigger verwendet einen Kalenderplan. Dadurch können Pipelines periodisch oder mit einem kalenderbasierten Wiederholungsmuster (etwa jeden Montag um 18:00 Uhr und jeden Donnerstag um 21:00 Uhr) geplant werden. Weitere Informationen finden Sie unter [Pipelineausführung und -trigger](concepts-pipeline-execution-triggers.md).\n\n### <a name=\"can-i-pass-parameters-to-a-pipeline-run\"></a>Kann ich Parameter an eine Pipelineausführung übergeben?\n\nJa. Parameter sind ein wichtiges Konzept in Data Factory. Sie können Parameter auf Pipelineebene definieren und Argumente übergeben, während die Pipelineausführung bei Bedarf oder mithilfe eines Triggers gestartet wird.  \n\n### <a name=\"can-i-define-default-values-for-the-pipeline-parameters\"></a>Kann ich Standardwerte für die Pipelineparameter definieren?\n\nJa. Sie können Standardwerte für die Parameter in den Pipelines definieren.\n\n### <a name=\"can-an-activity-in-a-pipeline-consume-arguments-that-are-passed-to-a-pipeline-run\"></a>Kann eine Aktivität in einer Pipeline Argumente verarbeiten, die an eine Pipelineausführung übergeben werden?\n\nJa. Jede Aktivität innerhalb der Pipeline kann den Parameterwert verwenden, der über das `@parameter`-Konstrukt an die Pipelineausführung übergeben wird. \n\n### <a name=\"can-an-activity-output-property-be-consumed-in-another-activity\"></a>Kann die Ausgabeeigenschaft einer Aktivität in einer anderen Aktivität verwendet werden?\n\nJa. Die Ausgabe einer Aktivität kann mit dem `@activity`-Konstrukt in einer nachfolgenden Aktivität verwendet werden.\n \n### <a name=\"how-do-i-gracefully-handle-null-values-in-an-activity-output\"></a>Wie werden NULL-Werte ordnungsgemäß in der Ausgabe einer Aktivität behandelt?\n\nNULL-Werte können mithilfe des `@coalesce`-Konstrukts in den Ausdrücken ordnungsgemäß behandelt werden.\n"
  - question: >
      Zuordnen von Datenflüssen
    answer: "### <a name=\"i-need-help-troubleshooting-my-data-flow-logic-what-info-do-i-need-to-provide-to-get-help\"></a>Ich brauche Hilfe bei der Behandlung von Problemen mit meiner Datenflusslogik. Welche Informationen muss ich angeben, um Hilfe zu erhalten?\n\nWenn Microsoft Hilfe oder eine Problembehandlung für Datenflüsse bereitstellt, geben Sie bitte die ADF-Pipeline Support-Dateien an.\nDiese Zip-Datei beinhaltet das CodeBehind-Skript aus dem Datenflussdiagramm. Klicken Sie in der ADF-Benutzeroberfläche auf **...** neben Pipeline und klicken Sie dann auf **Support-Dateien herunterladen**.\n\n### <a name=\"how-do-i-access-data-by-using-the-other-90-dataset-types-in-data-factory\"></a>Wie kann ich mit den anderen 90 Datasettypen in Data Factory auf Daten zugreifen?\n\nMit dem Feature „Zuordnungsdatenfluss“ können aktuell Azure SQL-Datenbank, Azure Synapse Analytics, durch Trennzeichen getrennte Textdateien aus Azure Blob Storage oder Azure Data Lake Storage Gen2 sowie Parquet-Dateien aus Blob Storage oder Data Lake Storage Gen2 nativ für Quelle und Senke verwendet werden. \n\nVerwenden Sie die Kopieraktivität, um Daten aus einem der anderen Connectors bereitzustellen, und führen Sie dann eine Datenflussaktivität aus, um Daten nach der Bereitstellung zu transformieren. So führt Ihre Pipeline beispielsweise zuerst einen Kopiervorgang nach Blob Storage aus, und anschließend verwendet eine Datenflussaktivität ein Dataset in der Quelle, um diese Daten zu transformieren.\n\n### <a name=\"is-the-self-hosted-integration-runtime-available-for-data-flows\"></a>Ist die selbstgehostete Integration Runtime für Datenflüsse verfügbar?\n\nDie selbstgehostete IR ist ein ADF-Pipelinekonstrukt, das Sie mit der Kopieraktivität zum Abrufen oder Verschieben von Daten in und aus lokalen oder VM-basierten Datenquellen und -senken verwenden können. Die virtuellen Computer, die Sie für eine selbstgehostete IR verwenden, können auch innerhalb desselben VNET wie Ihre geschützten Datenspeicher platziert werden, um über ADF auf diese Datenspeicher zugreifen zu können. Mit Datenflüssen erzielen Sie dieselben Endergebnisse, wenn Sie stattdessen die Azure IR mit verwaltetem VNET verwenden.\n\n### <a name=\"does-the-data-flow-compute-engine-serve-multiple-tenants\"></a>Bedient die Datenfluss-Computerengine mehrere Mandanten?\n\nCluster werden nie gemeinsam genutzt. Wir garantieren die Isolation für jede Auftragsausführung in Produktionsläufen. Bei einem Debugszenario erhält eine einzige Person einen Cluster, und alle Debuggingfehler werden in diesen Cluster verschoben, der von dem betreffenden Benutzer gestartet wird.\n\n### <a name=\"is-there-a-way-to-write-attributes-in-cosmos-db-in-the-same-order-as-specified-in-the-sink-in-adf-data-flow\"></a>Gibt es eine Möglichkeit, Attribute in Cosmos DB in derselben Reihenfolge zu schreiben, in der sie in der Senke im ADF-Datenfluss angegeben sind?    \n\nBei Cosmos DB ist das zugrunde liegende Format jedes Dokuments ein JSON-Objekt. Hierbei handelt es sich um einen ungeordneten Satz von Name-Wert-Paaren, sodass die Reihenfolge nicht reserviert werden kann. \n\n### <a name=\"why-a-user-is-unable-to-use-data-preview-in-the-data-flows\"></a>Warum kann ein Benutzer die Datenvorschau in den Datenflüssen nicht verwenden? \n\nSie sollten die Berechtigungen für die benutzerdefinierte Rolle überprüfen. An der Vorschau von Dataflowdaten sind mehrere Aktionen beteiligt. Zunächst überprüfen Sie den Netzwerkdatenverkehr während des Debuggens in Ihrem Browser. Folgen Sie allen Aktionen. Ausführliche Informationen finden Sie unter [Ressourcenanbieter](../role-based-access-control/resource-provider-operations.md#microsoftdatafactory).\n\n### <a name=\"in-adf-can-i-calculate-value-for-a-new-column-from-existing-column-from-mapping\"></a>Kann ich in ADF den Wert für eine neue Spalte anhand einer vorhandenen Spalte aus der Zuordnung berechnen?   \n\nSie können die Ableitungstransformation im Zuordnungsdatenfluss verwenden, um eine neue Spalte für die gewünschte Logik zu erstellen. Beim Erstellen einer abgeleiteten Spalte können Sie entweder eine neue Spalte generieren oder eine vorhandene Spalte aktualisieren. Geben Sie im Textfeld Spalte die Spalte ein, die Sie erstellen. Wenn Sie eine vorhandene Spalte in Ihrem Schema überschreiben möchten, können Sie die Dropdownliste für Spalten verwenden. Um den Ausdruck der abgeleiteten Spalte zu erstellen, klicken Sie auf das Textfeld Ausdruck eingeben. Sie können entweder mit dem Eingeben des Ausdrucks beginnen oder den Ausdrucks-Generator öffnen, um die Logik zu erstellen.\n\n### <a name=\"why-mapping-data-flow-preview-failing-with-gateway-timeout\"></a>Warum tritt bei der Vorschau des Zuordnungsdatenflusses ein Fehler mit Gatewaytimeout auf? \n\nVersuchen Sie, einen größeren Cluster zu verwenden, und setzen Sie die Zeilengrenzwerte in den Debugeinstellungen auf einen kleineren Wert, um die Größe der Debugausgabe zu reduzieren.\n\n### <a name=\"how-to-parameterize-column-name-in-dataflow\"></a>Wie wird der Spaltenname im Datenfluss parametrisiert?\n\nDer Spaltenname kann ähnlich wie andere Eigenschaften parametrisiert werden. Wie bei einer abgeleiteten Spalte kann der Kunde **$ColumnNameParam = toString(byName($myColumnNameParamInData))** verwenden. Diese Parameter können von der Pipelineausführung an Datenflüsse weitergegeben werden.\n\n### <a name=\"the-data-flow-advisory-about-ttl-and-costs\"></a>Die Datenflussempfehlung zu TLL und Kosten\n\nHilfreiche Informationen finden Sie in diesem Dokument zur Problembehandlung: [Anleitung zur Leistung und Optimierung der Mapping Data Flow-Funktion – Gültigkeitsdauer](../data-factory/concepts-integration-runtime-performance.md#time-to-live).\n"
  - question: >
      Power Query Data Wrangling
    answer: "### <a name=\"what-are-the-supported-regions-for-data-wrangling\"></a>Welche Regionen werden für den Wrangling-Datenfluss unterstützt?\n\nData Factory ist in den folgenden [Regionen](https://azure.microsoft.com/global-infrastructure/services/?products=data-factory) verfügbar.\nDie Funktion „Power Query“ ist in allen Datenflussregionen verfügbar. Wenn das Feature in Ihrer Region nicht verfügbar ist, wenden Sie sich an den Support.\n               \n### <a name=\"what-is-the-difference-between-mapping-data-flow-and-power-query-actvity-data-wrangling\"></a>Worin besteht der Unterschied zwischen der Zuordnungsdatenfluss-Funktion und der Power-Query-Aktivität (Data Wrangling)?\n\nMit Zuordnungsdatenflüssen können Sie Daten nach Maß transformieren, ohne Code schreiben zu müssen. Sie können einen Datentransformationsauftrag auf der Datenflusscanvas entwerfen, indem Sie eine Reihe von Transformationen erstellen. Beginnen Sie mit einer beliebigen Anzahl von Quelltransformationen, gefolgt von Datentransformationsschritten. Vervollständigen Sie Ihren Datenfluss mit einer Senke, damit Ihre Ergebnisse an ein Ziel gelangen. Der Zuordnungsdatenfluss eignet sich besonders für das Zuordnen und Transformieren von Daten mit bekannten und unbekannten Schemas in den Senken und Quellen.\n\nPower-Query Data Wrangling ermöglicht Ihnen das Vorbereiten und Durchsuchen agiler Daten mithilfe des Power Query Online-Mashup-Editors in jeder Größenordnung per Spark-Ausführung. Mit dem Anstieg von Data Lakes ist es manchmal erforderlich, ein Dataset zu durchsuchen oder ein Dataset im Lake zu erstellen. Sie nehmen keine Zuordnung zu einem bekannten Ziel vor.\n\n### <a name=\"supported-sql-types\"></a>Unterstützte SQL-Typen\n\nDas Power-Query Data Wrangling unterstützt die folgenden Datentypen in SQL. Bei Verwendung eines nicht unterstützten Datentyps wird ein Validierungsfehler angezeigt.\n\n* short\n* double\n* real\n* float\n* char\n* NCHAR\n* varchar\n* NVARCHAR\n* integer\n* INT\n* bit\n* boolean\n* SMALLINT\n* TINYINT\n* BIGINT\n* long\n* text\n* date\n* datetime\n* datetime2\n* smalldatetime\n* timestamp\n* UNIQUEIDENTIFIER\n* Xml\n"
additionalContent: "\n## <a name=\"next-steps\"></a>Nächste Schritte\n\nSchritt-für-Schritt-Anleitungen zum Erstellen einer Data Factory-Instanz finden Sie in den folgenden Tutorials:\n        \n- [Schnellstart: Erstellen einer Data Factory](quickstart-create-data-factory-dot-net.md)\n- [Tutorial: Kopieren von Daten in die Cloud](tutorial-copy-data-dot-net.md)"
